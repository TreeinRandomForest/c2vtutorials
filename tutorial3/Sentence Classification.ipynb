{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38aee59e",
   "metadata": {},
   "source": [
    "Goal: To demonstrate the neural net architecture for code2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7aff992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AmazonReviewFull\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b272dd",
   "metadata": {},
   "source": [
    "Get some C++/python source code\n",
    "\n",
    "Generate ASTs\n",
    "\n",
    "Generate AST paths\n",
    "\n",
    "node_to_index, index_to_node: IfStmt -> 0, 0 -> IfStmt\n",
    "\n",
    "merge_sort -> AST -> 15 paths -> file: [1, 5, 2, 3, ...], [10, 54, ...], merge_sort\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "model.py: python model.py train DATA_LOC -> persisted stuff, validation score (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9efce3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10e00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_train = AmazonReviewFull(root='../', split='train')\n",
    "dp_test = AmazonReviewFull(root='../', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ba626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dl_train = DataLoader(dp_train, batch_size=batch_size)\n",
    "dl_test = DataLoader(dp_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9d35ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = next(iter(dl_train)) #sample one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d1c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors, but great story I was a dissapointed to see errors on the back cover, but since I paid for the book I read it anyway. I have to say I love it. I couldn't put it down. I read the whole book in two hours. I say buy it. I say read it. It is sad, but it gives an interesting point of view on church today. We spend too much time looking at the faults of others. I also enjoyed beloved.Sincerly,Jaylynn R\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "print(X[7]) #review\n",
    "print(y[7]) #rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c043675",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a79ed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['a', 'also', 'an', 'anyway', 'at', 'back', 'beloved', 'book',\n",
       "        'but', 'buy', 'church', \"couldn't\", 'cover', 'dissapointed',\n",
       "        'down', 'enjoyed', 'errors', 'faults', 'for', 'gives', 'great',\n",
       "        'have', 'hours', 'i', 'in', 'interesting', 'is', 'it', 'looking',\n",
       "        'love', 'much', 'of', 'on', 'others', 'paid', 'point', 'put', 'r',\n",
       "        'read', 'sad', 'say', 'see', 'since', 'sincerlyjaylynn', 'spend',\n",
       "        'story', 'the', 'time', 'to', 'today', 'too', 'two', 'view', 'was',\n",
       "        'we', 'whole'], dtype='<U15'),\n",
       " array([ 1,  1,  1,  1,  1,  1,  1,  2,  3,  1,  1,  1,  1,  1,  1,  1,  2,\n",
       "         1,  1,  1,  1,  1,  1, 10,  1,  1,  1,  7,  1,  1,  1,  2,  2,  1,\n",
       "         1,  1,  1,  1,  3,  1,  3,  1,  1,  1,  1,  1,  4,  1,  2,  1,  1,\n",
       "         1,  1,  1,  1,  1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test processing of single sentence\n",
    "np.unique(np.concatenate([s.strip().replace(',','').split() for s in X[7].lower().split('.')]), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cd7bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "#build dictionary of word counts i.e. word -> occurrence count\n",
    "\n",
    "def get_counts(dl_train):\n",
    "    counts_dict = {}\n",
    "    for y, X_batch in dl_train: #loop over batch\n",
    "        #get unique words and counts from each sentence\n",
    "        for X in X_batch: #loop over sentences in batch\n",
    "            words, counts = np.unique(np.concatenate([s.strip().replace(',','').split() for s in X.lower().split('.')]), return_counts=True)\n",
    "\n",
    "            for idx, w in enumerate(words):\n",
    "                if w not in counts_dict:\n",
    "                    counts_dict[w] = 0\n",
    "\n",
    "                counts_dict[w] += counts[idx]\n",
    "    return counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7c2a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-computed word counts...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('train_word_counts.pkl'):\n",
    "    print('Computing word counts...')\n",
    "    counts_dict = get_counts(dl_train)\n",
    "    pickle.dump(counts_dict, open('train_word_counts.pkl','wb'))\n",
    "else:\n",
    "    print('Reading pre-computed word counts...')\n",
    "    counts_dict = pickle.load(open('train_word_counts.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2c93a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print entries with decreasing word counts\n",
    "counts_sorted = sorted(counts_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6923530b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff300084cd0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([s[1] for s in counts_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0b861ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff3000172e0>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([s[1] for s in counts_sorted[0:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "336e1c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323521"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df87e2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position from top, word, count\n",
      "1 ('and', 6300311)\n",
      "10 ('for', 2704868)\n",
      "100 ('am', 307976)\n",
      "1000 ('table', 22410)\n",
      "10000 ('goodwill', 1025)\n",
      "100000 ('waterfowl', 26)\n"
     ]
    }
   ],
   "source": [
    "print('position from top, word, count')\n",
    "for i in np.power(10, np.arange(6)):\n",
    "    print(i, counts_sorted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d323310",
   "metadata": {},
   "source": [
    "Out of ~2.3 million words, even the 100,000th word has only a count of 26. Let's truncate our word list here (feel free to choose a smaller threshold for the exercise), and remove all less frequent words i.e. replace with ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a09e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = dict(counts_sorted[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "685bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = dict(zip(valid_words.keys(), np.arange(len(valid_words))))\n",
    "i2w = dict(zip(np.arange(len(valid_words)), valid_words.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1faa08a",
   "metadata": {},
   "source": [
    "### Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929247e8",
   "metadata": {},
   "source": [
    "From now on, we will work with just one batch of data or even just one data point i.e. one review. The idea is to put a model architecture together that we expect to be able to predict the ratings. In other words, we want an encoder that will encode a paragraph i.e. a sequence of sentences. There are multiple ways of doing this but we will focus on a model similar to code2seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79796113",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = next(iter(dl_train)) #sample one batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b68a8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "review = []\n",
    "for s in X[idx].lower().replace(',', '').split('.'): #loop over sentences\n",
    "    sent = []\n",
    "    for w in s.split(): #loop over words\n",
    "        if w in valid_words:\n",
    "            sent.append(w2i[w])\n",
    "    review.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6938e66c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1202,\n",
       "  13,\n",
       "  31,\n",
       "  95,\n",
       "  2,\n",
       "  12,\n",
       "  3,\n",
       "  1491,\n",
       "  4,\n",
       "  126,\n",
       "  1202,\n",
       "  17,\n",
       "  0,\n",
       "  117,\n",
       "  343,\n",
       "  13,\n",
       "  189,\n",
       "  2,\n",
       "  714,\n",
       "  10,\n",
       "  0,\n",
       "  18,\n",
       "  2,\n",
       "  52,\n",
       "  6,\n",
       "  772],\n",
       " [2, 19, 4, 153, 2, 85, 6],\n",
       " [2, 357, 160, 6, 172],\n",
       " [2, 52, 0, 290, 18, 9, 118, 497],\n",
       " [2, 153, 92, 6],\n",
       " [2, 153, 52, 6],\n",
       " [6, 7, 738, 13, 6, 501, 38, 190, 340, 5, 710, 17, 1578, 639],\n",
       " [78, 682, 72, 65, 59, 176, 34, 0, 7135, 5, 314],\n",
       " [2, 89, 306, 4897],\n",
       " [1529]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ea305",
   "metadata": {},
   "source": [
    "In the above example, we performed the following steps:\n",
    "\n",
    "* Select a review\n",
    "\n",
    "* Split into sentences\n",
    "\n",
    "* For each word in the sentence, (a) check if the word is a valid (frequently occuring) word. if not, remove the word, (b) map the valid word to its index using w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545aa50",
   "metadata": {},
   "source": [
    "Our goal is to motivate the architecture for code2seq. If we were just solving the review rating problem, one could treat the whole review as a sequence of tokens since there is a time order imposed on the sentences. For AST paths though, there is no such order. So we have to treat each AST path on an equivalent footing. This motivates the idea of using one encoder to encode each sentence separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043e6b3",
   "metadata": {},
   "source": [
    "#### Padding and packing\n",
    "\n",
    "PyTorch operations benefit from SIMD for matrix operations. Sequences, on the other hand, are generally of varying lengths and not packed in matrices. PyTorch has a few functions to \"pad\" and \"pack\" sequences so they can be treated as matrices for efficient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429acf58",
   "metadata": {},
   "source": [
    "Padding is a simple idea. Extend every sequence to have a length equal to the longest sequence by inserting extra symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "229ceb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1202,    2,    2,    2,    2,    2,    6,   78,    2, 1529],\n",
       "        [  13,   19,  357,   52,  153,  153,    7,  682,   89,   -1],\n",
       "        [  31,    4,  160,    0,   92,   52,  738,   72,  306,   -1],\n",
       "        [  95,  153,    6,  290,    6,    6,   13,   65, 4897,   -1],\n",
       "        [   2,    2,  172,   18,   -1,   -1,    6,   59,   -1,   -1],\n",
       "        [  12,   85,   -1,    9,   -1,   -1,  501,  176,   -1,   -1],\n",
       "        [   3,    6,   -1,  118,   -1,   -1,   38,   34,   -1,   -1],\n",
       "        [1491,   -1,   -1,  497,   -1,   -1,  190,    0,   -1,   -1],\n",
       "        [   4,   -1,   -1,   -1,   -1,   -1,  340, 7135,   -1,   -1],\n",
       "        [ 126,   -1,   -1,   -1,   -1,   -1,    5,    5,   -1,   -1],\n",
       "        [1202,   -1,   -1,   -1,   -1,   -1,  710,  314,   -1,   -1],\n",
       "        [  17,   -1,   -1,   -1,   -1,   -1,   17,   -1,   -1,   -1],\n",
       "        [   0,   -1,   -1,   -1,   -1,   -1, 1578,   -1,   -1,   -1],\n",
       "        [ 117,   -1,   -1,   -1,   -1,   -1,  639,   -1,   -1,   -1],\n",
       "        [ 343,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  13,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 189,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [   2,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 714,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  10,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [   0,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  18,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [   2,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  52,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [   6,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 772,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding sequence - notice the extra -1 values in each column. Each column is one sentence\n",
    "nn.utils.rnn.pad_sequence([torch.tensor(r) for r in review], padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9fa2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 10])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(review))\n",
    "nn.utils.rnn.pad_sequence([torch.tensor(r) for r in review], padding_value=-1).shape #(max sequence length, number of sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36f38229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 26])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_sequence([torch.tensor(r) for r in review], batch_first=True, padding_value=-1).shape #(number of sequences, max length of sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2315e",
   "metadata": {},
   "source": [
    "Packing \"flattens\" all the data (see matrix above and ignore the -1 values) into a tensor but keeps track of each row in the batch_sizes variable. In the example below, \"data\" is a list of all values in the sequences in \"review\". \"batch_sizes\" indicates that the first 10 entries belong to row 1 (or time-step 1), the next 9 entries belong to row/time-step 2, etc.\n",
    "\n",
    "PyTorch RNNs (and LSTMs, GRUs) accept packed sequences as inputs as we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd5a7815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1202,    6,   78,    2,    2,    2,    2,    2,    2, 1529,   13,    7,\n",
       "         682,   52,   19,  357,  153,  153,   89,   31,  738,   72,    0,    4,\n",
       "         160,   92,   52,  306,   95,   13,   65,  290,  153,    6,    6,    6,\n",
       "        4897,    2,    6,   59,   18,    2,  172,   12,  501,  176,    9,   85,\n",
       "           3,   38,   34,  118,    6, 1491,  190,    0,  497,    4,  340, 7135,\n",
       "         126,    5,    5, 1202,  710,  314,   17,   17,    0, 1578,  117,  639,\n",
       "         343,   13,  189,    2,  714,   10,    0,   18,    2,   52,    6,  772]), batch_sizes=tensor([10,  9,  9,  9,  6,  5,  5,  4,  3,  3,  3,  2,  2,  2,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([0, 6, 7, 3, 1, 2, 4, 5, 8, 9]), unsorted_indices=tensor([0, 4, 5, 3, 6, 7, 1, 2, 8, 9]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pack sequence\n",
    "nn.utils.rnn.pack_sequence([torch.tensor(r) for r in review], enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d1657",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "Another useful notion is that of an embedding. In the sequences above, each word is mapped to a unique integer. We could pass these integers as input to the neural network but they impose an ordering on the words which are fundamentally unordered. Another option would be to map each integer i to a vector, v of length N (N = total number of words in the vocabulary) where v[i] = 1 and v[j] = 0, $j\\neq i$. This imposes the condition that two distinct words are orthogonal i.e. $v_i . v_j = \\delta_{ij}$, which is not correct either.\n",
    "\n",
    "Ideally, we would map each word to a vector $w \\mapsto v_{w}$ where semantically similar words $w_1, w_2$ map to close-by vectors i.e. with $v_{w_1}.v_{w_2} \\approx 1$ and dissimilar words $w_1, w_2$ map to orthgonal vectors i.e. $v_{w_1}.v_{w_2} \\approx 0$.\n",
    "\n",
    "The deep learning answer to this problem is embeddings, which are hash tables where each integer/word is mapped to a vector of fixed dimensionality. These vectors are initialized randomly but are treated as weights during the training process. In other words, backpropagation computes the derivative of the loss with respect to the embedding vectors and gradient descent (Adam etc.) is used to modify them. The result is that the embeddings are learned based on the task and often satisfy the property of similarity mentioned in the previous paragraph.\n",
    "\n",
    "Using embeddings is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "78210195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1352, -0.0457, -1.4809, -0.7751,  0.3591,  0.3170, -0.9488,  1.5229,\n",
       "         0.7727, -0.1353,  0.2212, -0.5189, -0.1412,  0.5595,  0.8167,  0.2851,\n",
       "         0.6403, -0.9905, -1.1056,  1.7136,  0.5320, -1.2148, -1.0359,  0.7805,\n",
       "        -0.7933,  0.5040,  0.0517,  0.1210, -1.1327, -0.9171,  0.8764, -0.4645],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=10, embedding_dim=32)\n",
    "\n",
    "emb(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "28530bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5617, -2.6761, -1.1619, -1.2152, -0.2980,  0.8669, -0.1940,  1.8178,\n",
       "        -1.3877, -2.5781, -0.0604,  1.6899, -0.4111,  1.3562,  0.8966,  0.9183,\n",
       "        -1.0733, -1.1135, -0.4686, -0.4965, -0.0448,  0.9987, -1.1136, -1.2014,\n",
       "         1.1637,  0.1104,  2.2933, -0.2891, -0.9864,  0.9162, -0.1472,  0.0468],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7a5e873a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2449022/1647148960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#doesn't exist since we specified num_embeddings = 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "emb(torch.tensor(10)) #doesn't exist since we specified num_embeddings = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f078ef",
   "metadata": {},
   "source": [
    "The general flow is to do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e0481",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the sequences (sentences in the review) and map each token to a unique integer based on the w2i dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6aba2e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1202,\n",
       "  13,\n",
       "  31,\n",
       "  95,\n",
       "  2,\n",
       "  12,\n",
       "  3,\n",
       "  1491,\n",
       "  4,\n",
       "  126,\n",
       "  1202,\n",
       "  17,\n",
       "  0,\n",
       "  117,\n",
       "  343,\n",
       "  13,\n",
       "  189,\n",
       "  2,\n",
       "  714,\n",
       "  10,\n",
       "  0,\n",
       "  18,\n",
       "  2,\n",
       "  52,\n",
       "  6,\n",
       "  772],\n",
       " [2, 19, 4, 153, 2, 85, 6],\n",
       " [2, 357, 160, 6, 172],\n",
       " [2, 52, 0, 290, 18, 9, 118, 497],\n",
       " [2, 153, 92, 6],\n",
       " [2, 153, 52, 6],\n",
       " [6, 7, 738, 13, 6, 501, 38, 190, 340, 5, 710, 17, 1578, 639],\n",
       " [78, 682, 72, 65, 59, 176, 34, 0, 7135, 5, 314],\n",
       " [2, 89, 306, 4897],\n",
       " [1529]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 7\n",
    "review = []\n",
    "for s in X[idx].lower().replace(',', '').split('.'): #loop over sentences\n",
    "    sent = []\n",
    "    for w in s.split(): #loop over words\n",
    "        if w in valid_words:\n",
    "            sent.append(w2i[w])\n",
    "    review.append(sent)\n",
    "\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7d6c3d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 7, 5, 8, 4, 4, 14, 11, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "#this is a list of lengths of each sequence that we'll need later\n",
    "lens = [len(r) for r in review]\n",
    "print(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e295428",
   "metadata": {},
   "source": [
    "Step 2: pad the sequence with a special symbol, say 0. Note that we should have been more careful above and set w2i['pad'] = 0 and i2w[0] = 'pad'. Let's pretend this is the case for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aac5e5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 26])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_padded = nn.utils.rnn.pad_sequence([torch.tensor(r) for r in review], padding_value=0, batch_first=True)\n",
    "review_padded.shape #note the shape is (number of sequences, length of longest sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2eb1a179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1202,   13,   31,   95,    2,   12,    3, 1491,    4,  126, 1202,   17,\n",
       "            0,  117,  343,   13,  189,    2,  714,   10,    0,   18,    2,   52,\n",
       "            6,  772],\n",
       "        [   2,   19,    4,  153,    2,   85,    6,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  357,  160,    6,  172,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   52,    0,  290,   18,    9,  118,  497,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  153,   92,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  153,   52,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   6,    7,  738,   13,    6,  501,   38,  190,  340,    5,  710,   17,\n",
       "         1578,  639,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  78,  682,   72,   65,   59,  176,   34,    0, 7135,    5,  314,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   89,  306, 4897,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [1529,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abba50",
   "metadata": {},
   "source": [
    "Step 3: Use an embedding table to map each integer to its embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "531b8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(num_embeddings=len(w2i), embedding_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "125ab1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 26, 32])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(review_padded).shape #(num sequences/batch size, max length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccf048",
   "metadata": {},
   "source": [
    "Step 4: pack this sequence. the packed sequence can be used as an input to an RNN/LSTM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d8389e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.9094,  0.1121,  0.9202,  ..., -0.3945,  0.2835, -0.2500],\n",
       "        [ 0.5067, -0.0297, -0.1504,  ...,  1.2191,  0.0949,  0.9311],\n",
       "        [-0.4212,  0.1985,  0.2637,  ..., -1.1615, -0.5108,  0.9925],\n",
       "        ...,\n",
       "        [-0.5124, -0.5845, -0.8453,  ...,  1.4230,  0.3747, -0.2386],\n",
       "        [ 0.5067, -0.0297, -0.1504,  ...,  1.2191,  0.0949,  0.9311],\n",
       "        [ 0.8317,  0.0165, -1.0567,  ..., -0.4114,  0.1026,  0.3873]],\n",
       "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([10,  9,  9,  9,  6,  5,  5,  4,  3,  3,  3,  2,  2,  2,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([0, 6, 7, 3, 1, 2, 4, 5, 8, 9]), unsorted_indices=tensor([0, 4, 5, 3, 6, 7, 1, 2, 8, 9]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_padded_packed = nn.utils.rnn.pack_padded_sequence(emb(review_padded), lens, batch_first=True, enforce_sorted=False)\n",
    "review_padded_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cb7bb2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 32])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_padded_packed.data.shape #there's one embedded vector for each non-zero entry in the reviews_padded tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "494c4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15503b",
   "metadata": {},
   "source": [
    "Step 5: Pass packed sequence as an input to the LSTM. The output, hidden and cell states are all packed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "09f512d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "o, (h,c) = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)(review_padded_packed)\n",
    "print(type(o))\n",
    "print(type(h))\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286688d",
   "metadata": {},
   "source": [
    "Step 6: Unpack the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4d0ba071",
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l_o = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f628e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 26, 64])\n",
      "torch.Size([1, 10, 64])\n",
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "print(o.shape)\n",
    "print(h.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c93891",
   "metadata": {},
   "source": [
    "Step 7: For code2seq, we will work with the hidden states at the last time-step, $h$. It is sometimes useful to insert a small MLP (just one layer in this case) as a projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "29164636",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.Linear(64, 32)\n",
    "h_proj = proj(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "12b423b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40817c6a",
   "metadata": {},
   "source": [
    "This is the end of the encoding part. Step 8 would be a decoder that's application specific. In the sentence classification or function name prediction problem, the decoder would simply use h_proj to predict a distribution over n labels. E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "576d2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_nn = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 5)) #5 is number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "941c4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out_nn(h_proj.squeeze(1).sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a2469",
   "metadata": {},
   "source": [
    "Special note: Since there is no ordering on the sequence hidden activation vectors, h_proj, any network should ideally be permutation invariant. A general way of doing so is to add all the vectors together (see DeepSets paper to see that this is all one needs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c9e1d",
   "metadata": {},
   "source": [
    "The loss would be cross-entropy loss (essentially log likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "85ab3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d899c29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7229, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(pred, y[7].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b9988",
   "metadata": {},
   "source": [
    "### Model prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61c6876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim,\n",
    "                ):\n",
    "        \n",
    "        #1\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        \n",
    "        #2\n",
    "        self.encoder = nn.LSTM(input_size=emb_dim,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=False,\n",
    "                              )\n",
    "        \n",
    "        #3\n",
    "        self.lin_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        #4\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''This can be optimizer but stick to simple version for now\n",
    "        '''\n",
    "        \n",
    "        #x is a list of sequences of integers\n",
    "        e = self.emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522650b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
